{"nbformat": 4, "nbformat_minor": 5, "metadata": {}, "cells": [{"id": "a2ba79a2", "cell_type": "markdown", "source": "# Netflix Recommendation System\n## Feature Engineering, Modeling, and A/B Testing\n\nThis notebook uses the same cleaned dataset, then builds:\n- Feature engineering for content similarity\n- A content based recommender (TF-IDF + cosine similarity)\n- A simple collaborative filtering style baseline with simulated implicit events\n- A/B testing utilities (z-test, power and sample size)\n", "metadata": {}}, {"id": "958e0bd2", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "import pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.decomposition import TruncatedSVD\nfrom scipy.stats import norm\n\n# Theme\nNETFLIX_RED = \"#E50914\"\nRED_2 = \"#B20710\"\nDARK_BG = \"#141414\"\nMID_GRAY = \"#6D6D6D\"\nLIGHT_GRAY = \"#B3B3B3\"\n\nplt.rcParams[\"figure.facecolor\"] = DARK_BG\nplt.rcParams[\"axes.facecolor\"] = DARK_BG\nplt.rcParams[\"axes.edgecolor\"] = MID_GRAY\nplt.rcParams[\"axes.labelcolor\"] = LIGHT_GRAY\nplt.rcParams[\"xtick.color\"] = LIGHT_GRAY\nplt.rcParams[\"ytick.color\"] = LIGHT_GRAY\nplt.rcParams[\"text.color\"] = LIGHT_GRAY\nplt.rcParams[\"grid.color\"] = \"#2A2A2A\"\n\ndf = pd.read_csv(r\"/Users/miriamgarcia/Desktop/NetflixRecommendationSystem/cleaned_netflix_data.csv\")\ndf[\"duration\"] = pd.to_numeric(df[\"duration\"], errors=\"coerce\")\ndf.head()\n", "outputs": []}, {"id": "dbefe4c5", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "# Feature engineering for content similarity\n# keep this simple and explicit so it's easy to explain in interviews\ndf_fe = df.copy()\n\nfor col in [\"director\", \"cast\", \"country\", \"listed_in\", \"description\", \"title\"]:\n    if col in df_fe.columns:\n        df_fe[col] = df_fe[col].fillna(\"\")\n\ndf_fe[\"text_blob\"] = (\n    df_fe[\"title\"] + \" \" +\n    df_fe[\"listed_in\"] + \" \" +\n    df_fe[\"director\"] + \" \" +\n    df_fe[\"cast\"] + \" \" +\n    df_fe[\"country\"] + \" \" +\n    df_fe[\"description\"]\n)\n\n# quick sanity check\ndf_fe[[\"title\", \"text_blob\"]].head(3)\n", "outputs": []}, {"id": "ca6af3ea", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "# TF-IDF vectorization\ntfidf = TfidfVectorizer(stop_words=\"english\", max_features=8000, ngram_range=(1,2))\nX = tfidf.fit_transform(df_fe[\"text_blob\"])\n\nprint(\"TF-IDF matrix:\", X.shape)\n", "outputs": []}, {"id": "8df9655b", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "# Cosine similarity and a recommendation function\nsim = cosine_similarity(X, X)\n\ntitle_to_idx = pd.Series(df_fe.index, index=df_fe[\"title\"]).drop_duplicates()\n\ndef recommend_by_title(title, k=10):\n    if title not in title_to_idx:\n        return pd.DataFrame()\n\n    idx = int(title_to_idx[title])\n    scores = list(enumerate(sim[idx]))\n    scores = sorted(scores, key=lambda x: x[1], reverse=True)\n\n    top = [i for i,_ in scores[1:k+1]]\n    out = df_fe.loc[top, [\"title\", \"type\", \"release_year\", \"rating\", \"listed_in\"]].copy()\n    out[\"similarity\"] = [scores[i][1] for i in range(1, k+1)]\n    return out\n\nrecommend_by_title(df_fe[\"title\"].iloc[0], k=8)\n", "outputs": []}, {"id": "07a8d479", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "# Simulate implicit events (clicks) so we can demo evaluation + A/B testing style work\n# NOTE: real projects would use watch/click/impression logs\nrng = np.random.default_rng(7)\n\nn_users = 800\nn_items = len(df_fe)\n\n# simulate exposures\nn_rows = 25000\nevents = pd.DataFrame({\n    \"user_id\": rng.integers(0, n_users, size=n_rows),\n    \"item_id\": rng.integers(0, n_items, size=n_rows),\n})\n\n# build a simple probability of click using popularity proxies\n# small typo on purpose: popluarity\ngenre_pop = df_fe[\"listed_in\"].str.split(\", \").str[0].fillna(\"Unknown\")\ngenre_rate = genre_pop.value_counts(normalize=True)\n\nbase = 0.06\nevents[\"p_click\"] = base + 0.10 * events[\"item_id\"].map(\n    df_fe[\"listed_in\"].fillna(\"\").str.contains(\"Dramas\", case=False).astype(int)\n).fillna(0).astype(float)\n\nevents[\"p_click\"] = events[\"p_click\"].clip(0.01, 0.30)\nevents[\"clicked\"] = rng.binomial(1, events[\"p_click\"])\n\nevents.head()\n", "outputs": []}, {"id": "7b311329", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "# Build a user-item matrix (implicit feedback) and do a simple matrix factorization baseline\n# We'll use TruncatedSVD on a sparse matrix to keep deps light.\nfrom scipy.sparse import coo_matrix\n\nrows = events[\"user_id\"].to_numpy()\ncols = events[\"item_id\"].to_numpy()\ndata = events[\"clicked\"].to_numpy().astype(float)\n\nR = coo_matrix((data, (rows, cols)), shape=(n_users, n_items)).tocsr()\n\nsvd = TruncatedSVD(n_components=40, random_state=42)\nU = svd.fit_transform(R)          # user factors\nV = svd.components_.T             # item factors\n\n# score matrix is U @ V.T, but we won't materialize full matrix for memory\nprint(\"User factors:\", U.shape)\nprint(\"Item factors:\", V.shape)\n", "outputs": []}, {"id": "a19ead24", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "# Recommend items for a user (SVD scores)\ndef recommend_for_user(user_id, k=10):\n    user_vec = U[user_id]                    # (factors,)\n    scores = user_vec @ V.T                  # (items,)\n    top_idx = np.argsort(scores)[::-1][:k]\n    out = df_fe.loc[top_idx, [\"title\",\"type\",\"release_year\",\"rating\",\"listed_in\"]].copy()\n    out[\"score\"] = scores[top_idx]\n    return out\n\nrecommend_for_user(0, k=8)\n", "outputs": []}, {"id": "1e63594b", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "# Simple evaluation: hit-rate@K using a train/test split over events\n# We'll hold out a subset of clicked items per user.\nfrom collections import defaultdict\n\n# build per-user clicked lists\nclicked_items = events.loc[events[\"clicked\"] == 1].groupby(\"user_id\")[\"item_id\"].apply(list).to_dict()\n\n# sample one holdout click per user if possible\nholdout = {}\ntrain_clicked = defaultdict(set)\n\nfor u, items in clicked_items.items():\n    if len(items) >= 2:\n        it = int(rng.choice(items))\n        holdout[u] = it\n        for x in items:\n            if x != it:\n                train_clicked[u].add(int(x))\n\ndef hit_rate_at_k(k=10, n_eval=300):\n    users = list(holdout.keys())\n    rng.shuffle(users)\n    users = users[:min(n_eval, len(users))]\n\n    hits = 0\n    for u in users:\n        recs = recommend_for_user(u, k=k)\n        rec_item_ids = recs.index.to_numpy()\n        if holdout[u] in rec_item_ids:\n            hits += 1\n    return hits / len(users) if users else np.nan\n\nfor k in [5, 10, 20]:\n    print(\"hit_rate@\", k, \"=\", round(hit_rate_at_k(k=k), 4))\n", "outputs": []}, {"id": "a582b680", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "# A/B testing helpers (CTR z-test + sample size)\ndef z_test_proportions(clicks_a, n_a, clicks_b, n_b):\n    p1 = clicks_a / n_a\n    p2 = clicks_b / n_b\n    p_pool = (clicks_a + clicks_b) / (n_a + n_b)\n    se = np.sqrt(p_pool * (1 - p_pool) * (1/n_a + 1/n_b))\n    z = (p2 - p1) / se\n    p_value = 2 * (1 - norm.cdf(abs(z)))\n    return p1, p2, z, p_value\n\ndef sample_size_for_mde(baseline_ctr, mde_abs, alpha=0.05, power=0.8):\n    # two-sided test\n    z_alpha = norm.ppf(1 - alpha/2)\n    z_beta = norm.ppf(power)\n\n    p1 = baseline_ctr\n    p2 = baseline_ctr + mde_abs\n    p_bar = (p1 + p2) / 2\n\n    num = (z_alpha * np.sqrt(2*p_bar*(1-p_bar)) + z_beta * np.sqrt(p1*(1-p1) + p2*(1-p2)))**2\n    den = (p2 - p1)**2\n    return int(np.ceil(num / den))\n\n# quick example\nprint(\"Sample size per group for +1% CTR lift from 8%:\",\n      sample_size_for_mde(0.08, 0.01))\n", "outputs": []}, {"id": "5d5fc50f", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "# Simulate an A/B test comparing two ranking strategies\n# Control: popularity (proxy)\n# Treatment: content similarity boost\nrng = np.random.default_rng(11)\n\nn_users_sim = 4000\nn_impressions = 40000\n\n# simulate impressions\nab = pd.DataFrame({\n    \"variant\": rng.choice([\"control\",\"treatment\"], size=n_impressions, p=[0.5,0.5]),\n})\n\n# baseline CTR\nbase_ctr = 0.075\n\n# treatment gets a small lift\nab[\"p_click\"] = np.where(ab[\"variant\"] == \"treatment\", base_ctr + 0.008, base_ctr)\nab[\"clicked\"] = rng.binomial(1, ab[\"p_click\"])\n\nsummary = ab.groupby(\"variant\")[\"clicked\"].agg([\"sum\",\"count\"])\nsummary\n", "outputs": []}, {"id": "09f74ec1", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "# Run z-test\nclicks_a = int(summary.loc[\"control\",\"sum\"])\nn_a = int(summary.loc[\"control\",\"count\"])\nclicks_b = int(summary.loc[\"treatment\",\"sum\"])\nn_b = int(summary.loc[\"treatment\",\"count\"])\n\np1, p2, z, p_value = z_test_proportions(clicks_a, n_a, clicks_b, n_b)\n\nprint(\"Control CTR:\", round(p1, 4))\nprint(\"Treatment CTR:\", round(p2, 4))\nprint(\"z:\", round(z, 3))\nprint(\"p-value:\", round(p_value, 4))\n", "outputs": []}, {"id": "de09742a", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "# Visualize CTR results with Netflix colors\nctr = pd.DataFrame({\n    \"variant\": [\"control\",\"treatment\"],\n    \"ctr\": [p1, p2]\n})\n\nplt.figure(figsize=(6,4))\nsns.barplot(data=ctr, x=\"variant\", y=\"ctr\", palette=[MID_GRAY, NETFLIX_RED])\nplt.title(\"A/B Test CTR (Simulated)\")\nplt.xlabel(\"\")\nplt.ylabel(\"CTR\")\nplt.ylim(0, max(ctr[\"ctr\"]) * 1.35)\nplt.tight_layout()\nplt.show()\n", "outputs": []}]}